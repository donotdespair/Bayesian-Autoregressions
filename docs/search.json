[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Autoregressions",
    "section": "",
    "text": "Abstract. We present the basics of Bayesian estimation and inference for autoregressive models. The range of topics includes the natural conjugate analysis using normal-inverted-gamma 2 prior distribution and its extensions focusing on hierarchical modelling, conditional heteroskedasticity, and Student-t error terms. We focus on forecasting and sampling from the predictive density.\nKeywords. Autoregressions, Bayesian Inference, Forecasting, Heteroskedasticity, Hierarchical Modelling, Natural Conjugacy, Shrinkage Prior"
  },
  {
    "objectID": "index.html#the-arp-model",
    "href": "index.html#the-arp-model",
    "title": "Bayesian Autoregressions",
    "section": "The AR(\\(p\\)) model",
    "text": "The AR(\\(p\\)) model\nThe model is set for a univariate time series whose observation at time \\(t\\) is denoted by \\(y_t\\). It includes a \\(d\\)-vector \\(d_t\\) of deterministic terms and \\(p\\) lags of the dependent variable on the right-hand side of the model equation. It is complemented by error term \\(u_t\\) that, in this note, is zero-mean normally distributed with variance \\(\\sigma^2\\). Then the model equations are: \\[\\begin{align}\ny_t &= \\alpha_d' d_t + \\alpha_1 y_{t-1} + \\dots + \\alpha_p y_{t-p} + u_t\\\\\nu_t\\mid d_t, y_{t-1}, \\dots, y_{t-p} &\\sim\\mathcal{N}\\left(0, \\sigma^2\\right)\n\\end{align}\\] where \\(\\alpha_d\\) is a \\(d\\)-vector of coefficients on deterministic terms, and parameters \\(\\alpha_1,\\dots,\\alpha_p\\) are autoregressive slopes."
  },
  {
    "objectID": "index.html#matrix-notation-for-the-model",
    "href": "index.html#matrix-notation-for-the-model",
    "title": "Bayesian Autoregressions",
    "section": "Matrix notation for the model",
    "text": "Matrix notation for the model\nTo simplify the notation and the derivations introduce matrix notation for the model. Let \\(T\\) be the available sample size for the variable \\(y\\). Define a \\(T\\)-vector of zeros, \\(\\mathbf{0}_T\\), the identity matrix of order \\(T\\), \\(\\mathbf{I}_T\\), \\(T\\times1\\) vectors: \\[\\begin{align}\n\\mathbf{y} = \\begin{bmatrix} y_1\\\\ \\vdots \\\\ y_T\\end{bmatrix}, \\quad\n\\text{ and }\\quad\n\\mathbf{u} = \\begin{bmatrix} u_1\\\\ \\vdots \\\\ u_T\\end{bmatrix},\n\\end{align}\\] a \\(k\\times1\\) vector \\(\\mathbf{x}_t = \\begin{bmatrix}d_t' & y_{t-1}&\\dots& y_{t-} \\end{bmatrix}'\\), where \\(k=d+p\\), and a \\(T\\times k\\) matrix collecting the explanatory variables: \\[\\begin{align}\n\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1'\\\\ \\vdots \\\\ \\mathbf{x}_T'\\end{bmatrix}.\n\\end{align}\\] Collect the parameters of the conditional mean equation in a \\(k\\)-vector: \\[\\begin{align}\n\\boldsymbol\\alpha = \\begin{bmatrix} \\alpha_d'& \\alpha_1 & \\dots & \\alpha_p\\end{bmatrix}'.\n\\end{align}\\]\nThen the model can be written in a concise notation as: \\[\\begin{align}\n\\mathbf{y} &= \\mathbf{X} \\boldsymbol\\alpha + \\mathbf{u}\\\\\n\\mathbf{u}\\mid \\mathbf{X} &\\sim\\mathcal{N}_T\\left(\\mathbf{0}_T, \\sigma^2\\mathbf{I}_T\\right).\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#likelihood-function",
    "href": "index.html#likelihood-function",
    "title": "Bayesian Autoregressions",
    "section": "Likelihood function",
    "text": "Likelihood function\nThe model equations imply the predictive density of the data vector \\(\\mathbf{y}\\). To see this, consider the model equation as a linear transformation of a normal vector \\(\\mathbf{u}\\). Therefore, the data vector follows a multivariate normal distribution given by: \\[\\begin{align}\n\\mathbf{y}\\mid \\mathbf{X}, \\boldsymbol\\alpha, \\sigma^2 &\\sim\\mathcal{N}_T\\left(\\mathbf{X} \\boldsymbol\\alpha, \\sigma^2\\mathbf{I}_T\\right).\n\\end{align}\\]\nThis distribution determines the shape of the likelihood function that is defined as the sampling data density: \\[\\begin{align}\nL(\\boldsymbol\\alpha,\\sigma^2|\\mathbf{y}, \\mathbf{X})\\equiv p\\left(\\mathbf{y}\\mid \\mathbf{X}, \\boldsymbol\\alpha, \\sigma^2 \\right).\n\\end{align}\\]\nThe likelihood function that for the sake of the estimation of the parameters, and after plugging in data in place of matrices \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\), is considered a function of parameters \\(\\boldsymbol\\alpha\\) and \\(\\sigma^2\\) is given by: \\[\\begin{align}\nL(\\boldsymbol\\alpha,\\sigma^2|\\mathbf{y}, \\mathbf{X}) =\n(2\\pi)^{-\\frac{T}{2}}\\left(\\sigma^2\\right)^{-\\frac{T}{2}}\\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^2}(\\mathbf{y} - \\mathbf{X}\\boldsymbol\\alpha)'(\\mathbf{y} - \\mathbf{X}\\boldsymbol\\alpha)\\right\\}.\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#likelihood-as-normal-inverted-gamma-2",
    "href": "index.html#likelihood-as-normal-inverted-gamma-2",
    "title": "Bayesian Autoregressions",
    "section": "Likelihood as normal-inverted gamma 2",
    "text": "Likelihood as normal-inverted gamma 2\nIn order to facilitate deriving the posterior distribution, the likelihood function can be rewritten to a \\(\\mathcal{NIG}2\\)-distribution given by:\n\\[\\begin{align}\nL(\\boldsymbol\\alpha,\\sigma^2|\\mathbf{y}, \\mathbf{X}) &\\propto \\left(\\sigma^2\\right)^{-\\frac{T-(k+2)+k+2}{2}}\\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^2}\\left(\\boldsymbol\\alpha-\\boldsymbol{\\hat{\\alpha}}\\right)'\\mathbf{X}'\\mathbf{X}\\left(\\boldsymbol\\alpha-\\boldsymbol{\\hat{\\alpha}}\\right) \\right\\}\\\\\n&\\qquad\\times\\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^2}\\left(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\hat{\\alpha}}\\right)'\\left(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\hat{\\alpha}}\\right) \\right\\},\n\\end{align}\\]\nwhere \\(\\boldsymbol{\\hat{\\alpha}} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\\) in the maximum likelihood estimator of \\(\\boldsymbol\\alpha\\). It is now quite straight forward to identify the \\(\\mathcal{NIG}2\\)-kernel. By remembering that the \\(\\mathcal{NIG}2\\)-distribution is characterized by its four moments, we get the following outcome:\n\\[\\begin{align}\nL(\\boldsymbol\\alpha,\\sigma^2|\\mathbf{y}, \\mathbf{X}) = \\mathcal{NIG}2\\left(\\mu=\\boldsymbol{\\hat{\\alpha}},\\Sigma=\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1},s=\\left(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\hat{\\alpha}}\\right)'\\left(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\hat{\\alpha}}\\right),\\nu=T-k-2\\right)\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#normal-inverted-gamma-2-prior",
    "href": "index.html#normal-inverted-gamma-2-prior",
    "title": "Bayesian Autoregressions",
    "section": "Normal-inverted gamma 2 prior",
    "text": "Normal-inverted gamma 2 prior\nThe prior distribution of the natural conjugate is determined by the form of the distribution of the parameters implied by the likelihood function discussed above. The priors for the Normal-inverse gamma 2 distribution can thus be written as:\n\\[\\begin{align}\np(\\boldsymbol\\alpha,\\sigma^2) &= p(\\boldsymbol\\alpha|\\sigma^2)p(\\sigma^2)\\\\\n      p(\\boldsymbol\\alpha|\\sigma^2) &= \\mathcal{N}(\\underline{\\boldsymbol\\alpha},\\sigma^2\\underline{\\mathbf{V}}_{\\boldsymbol\\alpha})\\\\\n      p(\\sigma^2) & = \\mathcal{IG}2(\\underline{s},\\underline{\\nu})\n\\end{align}\\]\nUsing the distributions above, we can write the kernel of the \\(\\mathcal{NIG}2\\) prior as:\n\\[\\begin{align}\n\\mathcal{NIG}2_{k}(\\underline{\\boldsymbol\\alpha},\\underline{\\mathbf{V}}_{\\boldsymbol\\alpha}, \\underline{s}, \\underline{\\nu}) \\propto (\\sigma^2)^{-\\frac{\\underline{\\nu}+k+2}{2}}\\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^2}(\\boldsymbol\\alpha-\\underline{\\boldsymbol\\alpha})'\\underline{\\mathbf{V}}^{-1}_{\\boldsymbol\\alpha}(\\boldsymbol\\alpha-\\underline{\\boldsymbol\\alpha})\\right\\}\\exp\\left\\{-\\frac{1}{2}\\frac{\\underline{s}}{\\sigma^2}\\right\\}\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#normal-inverted-gamma-2-posterior",
    "href": "index.html#normal-inverted-gamma-2-posterior",
    "title": "Bayesian Autoregressions",
    "section": "Normal-inverted gamma 2 posterior",
    "text": "Normal-inverted gamma 2 posterior\nThe product of the prior distribution and the likelihood function as introduced above gives the posterior distribution given by:\n\\[\\begin{align}\np(\\boldsymbol\\alpha,\\sigma^2|\\mathbf{y},\\mathbf{X}) \\propto L(\\mathbf{y}|\\mathbf{X}, \\boldsymbol\\alpha, \\sigma^2)p(\\boldsymbol\\alpha,\\sigma^2) =  L(\\mathbf{y}|\\mathbf{X}, \\boldsymbol\\alpha, \\sigma^2)p( \\boldsymbol\\alpha| \\sigma^2)p(\\sigma^2)\n\\end{align}\\]\nNow plugging in the expressions for the likelihood and the prior distribution:\n\\[\\begin{align}\np(\\boldsymbol\\alpha,\\sigma^2|\\mathbf{y},\\mathbf{X}) &\\propto \\left(\\sigma^2\\right)^{-\\frac{T-(k-2)+k+2}{2}}\\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^2}\\left(\\boldsymbol\\alpha-\\boldsymbol{\\hat{\\alpha}}\\right)'\\mathbf{X}'\\mathbf{X}\\left(\\boldsymbol\\alpha-\\boldsymbol{\\hat{\\alpha}}\\right) \\right\\}\\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^2}\\left(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\hat{\\alpha}}\\right)'\\left(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\hat{\\alpha}}\\right) \\right\\} \\\\\n&\\times (\\sigma^2)^{-\\frac{\\underline{\\nu}+k+2}{2}}\\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^2}(\\boldsymbol\\alpha-\\underline{\\boldsymbol\\alpha})'\\underline{\\mathbf{V}}_{\\boldsymbol\\alpha}^{-1}(\\boldsymbol\\alpha-\\underline{\\boldsymbol\\alpha})\\right\\}\\exp\\left\\{-\\frac{1}{2}\\frac{\\underline{s}}{\\sigma^2}\\right\\}\n\\end{align}\\]\nBy collecting all terms and simplifying the expressions, the joint posterior distribution can be derived to be:\n\\[\\begin{align}\np(\\boldsymbol\\alpha,\\sigma^2|\\mathbf{y},\\mathbf{X}) &\\propto (\\sigma^2)^{-\\frac{T+\\underline{\\nu}+k+2}{2}}\\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^2} (\\boldsymbol\\alpha-\\overline{\\boldsymbol\\alpha})'\\overline{\\mathbf{V}}_{\\boldsymbol\\alpha}^{-1}(\\boldsymbol\\alpha-\\overline{\\boldsymbol\\alpha})\\right\\} \\times \\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^2}\\left(\\underline{s}-\\overline{\\boldsymbol\\alpha}'\\overline{\\mathbf{V}}^{-1}_{\\boldsymbol\\alpha}\\overline{\\boldsymbol\\alpha}+\\underline{\\boldsymbol\\alpha}'\\underline{\\mathbf{V}}^{-1}_{\\boldsymbol\\alpha}\\underline{\\boldsymbol\\alpha}+\\mathbf{y}'\\mathbf{y}\\right)\\right\\}\n\\\\&= (\\sigma^2)^{\\frac{\\overline{\\nu}+k+2}{2}}\\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^2} (\\boldsymbol\\alpha-\\overline{\\boldsymbol\\alpha})'\\overline{\\mathbf{V}}_{\\boldsymbol\\alpha}^{-1}(\\boldsymbol\\alpha-\\overline{\\boldsymbol\\alpha})\\right\\} \\times \\exp\\left\\{-\\frac{1}{2}\\frac{\\overline{s}}{\\sigma^2}\\right\\}\n\\end{align}\\]\nThis now fully defines the joint posterior distribution as is it is in a normal inverse gamma 2 form with its corresponding four moments:\n\\[\\begin{align}\np(\\boldsymbol\\alpha,\\sigma^2|\\mathbf{y},\\mathbf{X}) &= \\mathcal{NIG}2_{k}\\left(\\overline{\\boldsymbol\\alpha},\\overline{\\mathbf{V}}_{\\boldsymbol\\alpha},\\overline{s},\\overline{\\nu}\\right)\\\\\n\\overline{\\boldsymbol\\alpha} &= \\overline{\\mathbf{V}}_{\\boldsymbol\\alpha}( \\underline{\\mathbf{V}}^{-1}_{\\boldsymbol\\alpha}\\underline{\\boldsymbol\\alpha}+\\mathbf{X}'\\mathbf{y})\\\\\n\\overline{\\mathbf{V}}_{\\boldsymbol\\alpha} &=\\left(\\underline{\\mathbf{V}}^{-1}_{\\boldsymbol\\alpha}+\\mathbf{X}'\\mathbf{X}\\right)^{-1} \\\\\n\\overline{s} &= \\underline{s}-\\overline{\\boldsymbol\\alpha}'\\overline{\\mathbf{V}}^{-1}_{\\boldsymbol\\alpha}\\overline{\\boldsymbol\\alpha}+\\underline{\\boldsymbol\\alpha}'\\underline{\\mathbf{V}}^{-1}_{\\boldsymbol\\alpha}\\underline{\\boldsymbol\\alpha}+\\mathbf{y}'\\mathbf{y}\\\\\n\\overline{\\nu} &= \\underline{\\nu}+T\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#sampling-draws-from-the-posterior",
    "href": "index.html#sampling-draws-from-the-posterior",
    "title": "Bayesian Autoregressions",
    "section": "Sampling draws from the posterior",
    "text": "Sampling draws from the posterior\nWe start by generating a random walk, which can be used to validate that our estimation is indeed correct.\n\nT       = 100\nN       = 1\n\ny       = apply(matrix(rnorm(T * N), ncol = N), 2, cumsum)\ny       = as.matrix(y)\np       = 1\nd       = 1\n\nk    = p + d \n\nT     = nrow(y)\nY     = as.matrix(y[(p+1):T,])\nX     = cbind(rep(1, T - p), y[1:(T - p),])\n\nNow, defining our priors for \\(\\underline{\\boldsymbol\\alpha}\\), \\(\\underline{\\mathbf{V}}_{\\boldsymbol\\alpha}\\), \\(\\underline{s}\\) and \\(\\underline{\\nu}\\):\n\npriors = list(\n  alpha   = as.matrix(rep(0, k)),\n  Sigma   = diag(2),\n  S       = 1,\n  nu      = 3\n)\n\nand computing the function for the posterior parameters:\n\nposterior = function(y, priors){\n  Sigma.inv = t(X) %*% X + priors$Sigma\n  Sigma     = solve(Sigma.inv)\n  alpha     = Sigma %*% (t(X) %*% Y + solve(priors$Sigma) %*% priors$alpha)\n  S         = as.numeric(t(Y) %*% Y + priors$S + t(priors$alpha) %*% solve(priors$Sigma) %*% priors$alpha \n                     - t(alpha) %*% Sigma.inv %*% alpha)\n  nu        = T + priors$nu \n  \n  return(list(\n    Sigma  = Sigma,\n    alpha  = alpha,\n    S      = S,\n    nu     = nu\n  ))\n}\n\npost = posterior(y = y, priors = priors)\n\nWe are then able to do the estimation of our parameters using the Gibbs sampler provided below.\n\nposterior.draws = function(S, posterior){\n  Sigma2.posterior      = as.matrix(posterior$S / rchisq(S, posterior$nu))\n  alpha.posterior       = simplify2array(\n    lapply(1:S, function(i){\n      mvtnorm::rmvnorm(1, mean = posterior$alpha, sigma = Sigma2.posterior[i,] * posterior$Sigma)\n    })\n  )\n  output = cbind(t(alpha.posterior[1,,]), Sigma2.posterior)\n  return(output)\n}\n\ndraws = posterior.draws(S = 1000, posterior = post)"
  },
  {
    "objectID": "index.html#estimating-autoregressive-prior-shrinkage",
    "href": "index.html#estimating-autoregressive-prior-shrinkage",
    "title": "Bayesian Autoregressions",
    "section": "Estimating autoregressive prior shrinkage",
    "text": "Estimating autoregressive prior shrinkage\n\nInverted gamma 2 scale mixture of normal\nGiven the scalar scale follows an inverted gamma 2 distribution: \\[\\begin{align}\n\\kappa_{\\boldsymbol\\alpha} \\sim \\mathcal{IG}2 (\\underline{s}_{\\boldsymbol\\alpha},\\underline{\\nu}_{\\boldsymbol\\alpha})\n\\end{align}\\]\nWe have priors as: \\[\\begin{align}\n\\boldsymbol\\alpha | \\sigma^2, \\kappa_{\\boldsymbol\\alpha} &\\sim \\mathcal{N}_{k}(\\underline{\\boldsymbol\\alpha}, \\sigma^2 \\kappa_{\\boldsymbol\\alpha} \\underline{\\mathbf{V}} _{\\boldsymbol\\alpha}) \\\\\n\\\\ \\sigma^2 &\\sim \\mathcal{IG}2(\\underline{s},\\underline{\\nu}) \\\\\n\\\\ p(\\boldsymbol\\alpha, \\sigma^2, \\kappa_{\\boldsymbol\\alpha}) &= p(\\boldsymbol\\alpha | \\sigma^2, \\kappa_{\\boldsymbol\\alpha}) p(\\sigma^2) p(\\kappa_{\\boldsymbol\\alpha})\n\\end{align}\\]\nThen, the joint distribution of \\((\\boldsymbol\\alpha,\\sigma^2)\\) given \\(\\kappa_{\\boldsymbol\\alpha}\\) would follow a normal-inverted gamma 2 distribution: \\[\\begin{align*}\np\\left(\\boldsymbol\\alpha,\\sigma^2 \\mid \\kappa_{\\boldsymbol\\alpha}\\right) = \\mathcal{NIG}2_N(\\underline{\\boldsymbol\\alpha},\\kappa_{\\boldsymbol\\alpha} \\underline{\\mathbf{V}} _{\\boldsymbol\\alpha}, \\underline{s},\\underline{\\nu})\n\\end{align*}\\]\nA Gibbs Sampler can be applied using the following steps:\nInitialize \\(\\kappa_{\\boldsymbol\\alpha}\\) at \\(\\kappa_{\\boldsymbol\\alpha}^{(0)}\\).\nAt each iteration s:\n    Step 1. Draw \\((\\boldsymbol\\alpha,\\sigma^2)^{(s)} \\sim p(\\boldsymbol\\alpha,\\sigma^2|\\mathbf{X},\\mathbf{y},\\kappa_{\\boldsymbol\\alpha}^{(s-1)}) = \\mathcal{NIG}2(\\overline{\\boldsymbol\\alpha},\\overline{\\mathbf{V}},\\overline{s},\\overline{\\nu})\\).\n    Step 2. Draw \\(\\kappa_{\\boldsymbol\\alpha}^{(s)} \\sim p(\\kappa_{\\boldsymbol\\alpha}|\\mathbf{X},\\mathbf{y},\\boldsymbol\\alpha,\\sigma^2) = \\mathcal{IG}2(\\overline{s},\\overline{\\nu})\\).\nRepeat step 1 and 2 \\((S_1+S_2)\\) times and discard the first \\(S_1\\) draws.\nThe full conditional posterior of \\(\\kappa_{\\boldsymbol\\alpha}\\) is: \\[\\begin{gather*}\np(\\kappa_{\\boldsymbol\\alpha}|\\mathbf{X},\\mathbf{y},\\boldsymbol\\alpha,\\sigma^2) = L(\\mathbf{y}|\\mathbf{X},\\boldsymbol\\alpha,\\sigma^2) p(\\kappa_{\\boldsymbol\\alpha})  p(\\boldsymbol\\alpha|\\sigma^2,\\kappa_{\\boldsymbol\\alpha}) p(\\sigma^2) \\\\\n\n\\\\ \\propto p(\\kappa_{\\boldsymbol\\alpha}) p(\\boldsymbol\\alpha|\\sigma^2,\\kappa_{\\boldsymbol\\alpha}) \\\\\n\n\\\\ \\propto (\\kappa_{\\boldsymbol\\alpha})^{-\\frac{\\nu+2}{2}} exp \\left\\{ -\\frac{1}{2}\\frac{s}{\\kappa_{\\boldsymbol\\alpha}} \\right\\}\n\\times \\text{det}(\\sigma^2 \\kappa_{\\boldsymbol\\alpha} \\underline{\\mathbf{V}} _{\\boldsymbol\\alpha})^{-\\frac{1}{2}} exp \\left\\{ -\\frac{1}{2} (\\boldsymbol\\alpha-\\underline{\\boldsymbol\\alpha})^{'}(\\sigma^2 \\kappa_{\\boldsymbol\\alpha} \\underline{\\mathbf{V}} _{\\boldsymbol\\alpha})^{-1} (\\boldsymbol\\alpha-\\underline{\\boldsymbol\\alpha})\\right\\} \\\\\n\n\\\\ =  (\\kappa_{\\boldsymbol\\alpha})^{-\\frac{\\nu+2+k}{2}} exp \\left\\{ -\\frac{1}{2}\\frac{s}{\\kappa_{\\boldsymbol\\alpha}} \\right\\} exp \\left\\{ -\\frac{1}{2}\\frac{1}{\\kappa_{\\boldsymbol\\alpha}} (\\boldsymbol\\alpha-\\underline{\\boldsymbol\\alpha})^{'} (\\sigma^2 \\underline{\\mathbf{V}}_{\\boldsymbol\\alpha})^{-1} (\\boldsymbol\\alpha-\\underline{\\boldsymbol\\alpha})\\right\\}\n\\end{gather*}\\]\nin which we recognize a kernel of inverted gamma 2 distribution with parameters:\n\\[\\begin{align}\n\\overline{s}_{\\boldsymbol\\alpha} &= s + (\\boldsymbol\\alpha-\\underline{\\boldsymbol\\alpha})^{'} \\sigma^{-2} \\underline{\\mathbf{V}}_{\\boldsymbol\\alpha}^{-1} (\\boldsymbol\\alpha-\\underline{\\boldsymbol\\alpha}) \\\\\n\\overline{\\nu}_{\\boldsymbol\\alpha} &= \\nu + k\n\\end{align}\\]\nTo sample from the \\(\\mathcal{IG}2(s,\\nu)\\) distribution, the following code can be used:\n\n# Set parameters of IG2 distribution \ns &lt;- 1 \nnu &lt;- 1  \n# Draw one sample from IG2 distribution \nsample &lt;- s / rchisq(1, df = nu)\n\n\n\nGamma scale mixture of normal\n\nContributor: Yobin Timilsena\n\nAlternatively, the scalar scale \\(\\kappa_{\\boldsymbol\\alpha}\\) that premultiplies the covariance matrix of the normal prior for vector \\(\\boldsymbol\\alpha\\) can be assumed to have a hierarchical prior with a gamma distribution: \\[\n\\kappa_{\\boldsymbol\\alpha}|\\underline s_\\kappa, \\underline a_\\kappa \\sim \\mathcal{G}(\\underline s_\\kappa, \\underline a_\\kappa)\n\\] The following code creates a list kappa.priors to store priors on \\(\\kappa_\\boldsymbol\\alpha\\).\n\nkappa.priors = list(\n  a = 1,\n  s = .1\n)\n\nGiven the likelihood and priors above, we can obtain the full conditional posterior of \\(\\kappa_\\boldsymbol\\alpha\\) as \\[\n\\begin{align*}\n    p(\\kappa_{\\boldsymbol\\alpha} | \\bf y, \\bf X, \\boldsymbol\\alpha, \\sigma^2) & \\propto p(\\kappa_{\\boldsymbol\\alpha} | \\underline s_{\\boldsymbol\\kappa}, \\underline a_{\\boldsymbol\\kappa}) \\cdot p(\\boldsymbol\\alpha | \\kappa_{\\boldsymbol\\alpha}, \\sigma^2)\\\\\n    &\\qquad\\times (\\kappa_{\\boldsymbol\\alpha})^{\\underline a_{\\boldsymbol\\kappa} - 1} \\exp \\left\\{  - \\frac{\\kappa_{\\boldsymbol\\alpha}}{\\underline s_{\\boldsymbol\\kappa}} \\right\\} \\cdot \\det(\\sigma^2 \\kappa_{\\boldsymbol\\alpha} \\underline{\\bf V}_{\\boldsymbol\\alpha})^{-\\frac{1}{2}} exp \\left\\{ -\\frac{1}{2} (\\boldsymbol\\alpha-\\underline{\\boldsymbol\\alpha})^{'}(\\sigma^2 \\kappa_{\\boldsymbol\\alpha} \\underline{\\mathbf{V}}_{\\boldsymbol\\alpha})^{-1} (\\boldsymbol\\alpha-\\underline{\\boldsymbol\\alpha})\\right\\}  \\\\\n    & \\propto (\\kappa_{\\boldsymbol\\alpha})^{\\underline a_{\\boldsymbol\\kappa} - 1} \\exp \\left\\{  - \\frac{\\kappa_{\\boldsymbol\\alpha}}{\\underline s_{\\boldsymbol\\kappa}} \\right\\} \\cdot (\\sigma^2)^{-\\frac{K}{2}} (\\kappa_{\\boldsymbol\\alpha})^{-\\frac{K}{2}} \\det(\\underline{\\bf V}_{\\boldsymbol\\alpha})^{-\\frac{1}{2}} exp \\left\\{ -\\frac{1}{2 \\kappa_{\\boldsymbol\\alpha}} (\\boldsymbol\\alpha-\\underline{\\boldsymbol\\alpha})^{'} (\\sigma^2 \\underline{\\mathbf{V}}_{\\boldsymbol\\alpha})^{-1} (\\boldsymbol\\alpha-\\underline{\\boldsymbol\\alpha})\\right\\}  \\\\\n    & \\propto (\\kappa_{\\boldsymbol\\alpha})^{\\underline a_{\\boldsymbol\\kappa} - \\frac{K}{2} - 1} \\exp \\left\\{  -\\frac{1}{2} \\left( (\\boldsymbol\\alpha-\\underline{\\boldsymbol\\alpha})^{'} (\\sigma^2 \\underline{\\mathbf{V}}_{\\boldsymbol\\alpha})^{-1} (\\boldsymbol\\alpha-\\underline{\\boldsymbol\\alpha}) \\cdot \\frac{1}{\\kappa_{\\boldsymbol\\alpha}} + \\frac{2}{\\underline s_{\\boldsymbol\\kappa}} \\kappa_{\\boldsymbol\\alpha} \\right) \\right\\}\n\\end{align*}\n\\] which is a kernel for a Generalised Inverse Gaussian distribution with parameters: \\[\n\\begin{align*}\n    & \\overline\\lambda = \\underline a_{\\boldsymbol\\kappa} - \\frac{K}{2}\\\\\n    & \\overline\\chi = (\\boldsymbol\\alpha-\\underline{\\boldsymbol\\alpha})^{'} (\\sigma^2 \\underline{\\mathbf{V}}_{\\boldsymbol\\alpha})^{-1} (\\boldsymbol\\alpha-\\underline{\\boldsymbol\\alpha}) \\\\\n    & \\overline\\Psi = \\frac{2}{\\underline s_{\\boldsymbol\\kappa}}\n\\end{align*}\n\\] Hence, the full-conditional posterior distribution of \\(\\kappa_\\alpha\\) follows a Generalised Inverse Gaussian distribution. \\[\n\\kappa_{\\boldsymbol\\alpha} | \\bf y, \\bf X, {\\boldsymbol\\alpha}, \\sigma^2 \\sim \\mathcal{GIG}(\\overline\\lambda, \\overline\\chi, \\overline\\Psi)\n\\] Given the full conditional posterior of both \\(\\kappa_\\alpha\\) and \\((\\boldsymbol\\alpha, \\sigma^2)\\), we can implement the Gibbs Sampler with the following steps:\n\nInitialise \\(\\kappa_\\alpha^{(0)}\\).\nAt each iteration \\(s\\):\n\nStep 1: Draw \\((\\boldsymbol\\alpha,\\sigma^2)^{(s)} \\sim p(\\boldsymbol\\alpha,\\sigma^2|\\mathbf{X},\\mathbf{y},\\kappa_{\\boldsymbol\\alpha}^{(s-1)}) = \\mathcal{NIG}2(\\overline{\\boldsymbol\\alpha},\\overline{\\mathbf{V}},\\overline{s},\\overline{\\nu})\\).\nStep 2. Draw \\(\\kappa_{\\boldsymbol\\alpha}^{(s)} \\sim p(\\kappa_{\\boldsymbol\\alpha}|\\mathbf{X},\\mathbf{y},(\\boldsymbol\\alpha,\\sigma^2)^{(s)}) = \\mathcal{GIG}(\\lambda, \\chi, \\Psi)\\).\n\n\nThe Gamma.sampler function below implements the Gibbs Sampler using the aforementioned algorithm.\n\nGamma.sampler = function(S, posterior, priors, kappa.priors){\n  set.seed(12345)\n  S.burnin = 100\n  S.total = S + S.burnin\n  \n  #create matrices to store posterior draws\n  alpha.posterior  = array(NA,c(k,1,S.total))\n  kappa.posterior  = array(NA,c(1,S.total))\n  #initial value for kappa\n  kappa.posterior[1] = 10\n  \n  # Draw sigma^2 from IG2 outside the loop as it does not update iteratively \n  Sigma2.posterior = posterior$S / rchisq(n=S.total,df=posterior$nu)\n  \n  for (i in 1:(S.total)) {\n    # Plug in current kappa to update Sigma (or V in slides)\n    prior.Sigma.inv = solve(kappa.posterior[i] * priors$Sigma)\n    Sigma.inv = t(X) %*% X + prior.Sigma.inv\n    Sigma = solve(Sigma.inv)\n    \n    # Draw alpha from normal dist using sigma^2 and updated var-covar matrix Sigma\n    \n    alpha.posterior[,,i]  = t(mvtnorm::rmvnorm(n=1, mean = posterior$alpha, sigma = Sigma2.posterior[i] * Sigma))\n    \n    # Update parameters for kappa posterior\n    lambda = kappa.priors$a - k/2\n    chi    = t(alpha.posterior[,,i] - priors$alpha) %*% solve(Sigma2.posterior[i] * priors$Sigma) %*% (alpha.posterior[,,i] - priors$alpha)\n    Psi    = 2 / kappa.priors$s\n    \n    # Draw next period value for kappa from GIG distribution\n    if (i != S.total){\n      kappa.posterior[i+1] = GIGrvg::rgig(n=1, lambda = lambda, chi = chi, psi = Psi)\n    }\n  }\n  \n  # save output as a list \n  list(\n    alpha.posterior = alpha.posterior[,,((S.burnin+1):S.total)],\n    Sigma2.posterior = Sigma2.posterior[((S.burnin+1):S.total)],\n    kappa.posterior = kappa.posterior[((S.burnin+1):S.total)]\n  )\n}\nGammaScale.draws = Gamma.sampler(S = 1000, post, priors, kappa.priors)\n\nI compute and display the posterior means for \\(({\\boldsymbol\\alpha, \\sigma^2, \\kappa_{\\boldsymbol\\alpha}})\\).\n\nalpha.posterior.mean = rowMeans(GammaScale.draws$alpha.posterior)\nalpha.posterior.mean\n\n[1] -0.09659746  0.98313834\n\n\n\nSigma2.posterior.mean = mean(GammaScale.draws$Sigma2.posterior)\nSigma2.posterior.mean\n\n[1] 1.018755\n\n\n\nkappa.posterior.mean = mean(GammaScale.draws$kappa.posterior)\nkappa.posterior.mean\n\n[1] 0.2442665"
  },
  {
    "objectID": "index.html#estimating-error-term-variance-prior-scale",
    "href": "index.html#estimating-error-term-variance-prior-scale",
    "title": "Bayesian Autoregressions",
    "section": "Estimating error term variance prior scale",
    "text": "Estimating error term variance prior scale\nIn this section, we estimate the error term variance prior scale \\(\\underline{s}\\) that follows a Gamma distribution \\(G(\\underline{s}_s,\\underline{a}_s)\\) with scale \\(\\underline{s}_s\\) shape \\(\\underline{a}_s\\) that follow a probability density function equal to: \\[p(\\underline{s})=\\Gamma(\\underline{a}_s)^{-1}\\underline{s}_s^{-\\underline{a}_s}\\underline{s}^{\\underline{a}_s-1}\\exp\\left\\{-\\frac{\\underline{s}}{\\underline{s}_s}\\right\\}\\] In order to find our full conditional posterior of \\(\\underline{s}\\) write out its kernel as: \\[\\begin{align}\np(\\underline{s}|y,X,\\alpha,\\sigma^2) &\\propto\nL(y|X,\\alpha,\\sigma^2) \\times p(\\alpha|\\sigma^2) \\times p(\\sigma^2|\\underline{s}) \\times p(\\underline{s})\\\\\n&\\propto p(\\sigma^2|\\underline{s}) \\times p(\\underline{s})\\\\\n&\\propto \\underline{s}^{\\frac{\\underline{\\nu}}{2}}\n\\exp\\left\\{-\\frac{1}{2}\\frac{\\underline{s}}{\\sigma^2}\\right\\}\n\\underline{s}^{\\underline{a}_s-1}\\exp\\left\\{-\\frac{\\underline{s}}{\\underline{s}_s}\\right\\}\\\\\n&\\propto\n\\underline{s}^{\\frac{\\underline{\\nu}}{2}+\\underline{a}_s-1}\\exp\\left\\{-\\frac{\\underline{s}}{\n\\left[\\left(2\\sigma^2\\right)^{-1} + \\underline{s}_s^{-1}\\right]^{-1}\n} \\right\\}\n\\end{align}\\] from which we recognise a Gamma function \\(G(\\overline{a}_s, \\overline{s}_s)\\) with parameters: \\[\\begin{align}\n\\overline{a}_s&=\\frac{\\underline{\\nu}}{2}+\\underline{a}_s\\\\\n\\overline{s}_s&=\\left[\\left(2\\sigma^2\\right)^{-1} + \\underline{s}_s^{-1}\\right]^{-1}\n\\end{align}\\]\nIn order to obtain a sample from the posterior distribution we use a Gibbs sampler. We generate random draws from the joint posterior distribution and we update them at each iteration. In this case we exploit the following procedure:\nInitialize \\(\\underline{s}\\) at \\(\\underline{s}^{(0)}\\)\nAt each iteration s:\n\nDraw \\((\\boldsymbol\\alpha,\\sigma^2)^{(s)} \\sim p\\left(\\boldsymbol\\alpha,\\sigma^2 \\mid \\mathbf{y},\\mathbf{X}, \\boldsymbol\\alpha^{(s-1)}, {\\sigma^2}^{(s-1)}, \\underline{s}^{(s)}\\right)\\)\nDraw \\(\\underline{s}^{(s)} \\sim p(\\underline{s}\\mid\\mathbf{y},\\mathbf{X},\\boldsymbol\\alpha,\\sigma^2)\\)\n\nRepeat steps 1 and 2 for \\((S_1 + S_2)\\) times. Discard the first \\(S_1\\) repetitions. Return the output as \\(\\left \\{ \\boldsymbol\\alpha^{(s)}, \\sigma^{2(s)}, \\underline{s}^{(s)}\\right \\}^{S_1+S_2}_{s=S_1+1}\\).\nThe following script illustrates sampling from the full conditional posterior distribution of \\(\\underline{s}\\).\n\nnu = 1\nsigma2 = 10\n\n# define the prior hyper-parameters\ns.prior.s  &lt;- 0.01\na.prior.s  &lt;- 1\n\n#define the posteriors\na.bar.s &lt;- (nu / 2) + s.prior.s\ns.bar.s &lt;- 1 / (1/(2 * sigma2) + (1 / s.prior.s))\n\n#sample from the gamma distribution using rgamma function\ns.sigma &lt;- rgamma(1, shape = a.bar.s, scale = s.bar.s)"
  },
  {
    "objectID": "index.html#dummy-observation-prior",
    "href": "index.html#dummy-observation-prior",
    "title": "Bayesian Autoregressions",
    "section": "Dummy observation prior",
    "text": "Dummy observation prior\nIn this extended model, the system is augmented using the sum of coefficients and dummy initial observation prior. The idea for this way of setting a prior is to generate artificial new rows that augment data matrices \\(\\bf y\\) and \\(\\bf X\\). It is equivalent to assuming a normal-inverted gamma 2 prior whose parameters are determined by dummy observations augmenting the system.\nThe sum of coefficients prior takes the average of the lagged values and adds them to the basic equation. This is because these values are assumed to be a good forecast of future observations. The dummy initial observation prior adds a single dummy observation such that all values are set equal to the averages of initial conditions, up to a scaling factor.\nIn order to practically generate the additional rows the following steps should be taken.\nFirstly, the average of lagged values needs to be calculated, y.bar. This is done by finding the mean of the first \\(p\\) values in the data. Next, values of of the scaling factors need to be selected, where values equal to 1 are chosen typically.\nOnce this has been done, the rows can be created. In the univariate case this is simple, two rows are added to vector \\(\\bf y\\), the first equal to y.bar divided by the first scaling factor, \\(\\lambda_3\\), the second equal to th same value divided by the second scaling factor, \\(\\lambda_4\\).\nAdding rows to \\(\\bf X\\) is slightly more complicated. The additional rows are as follows. The values in the first row, corresponding to the sum of coefficients, are all equal to y.bar divided by \\(\\lambda_3\\), except the last column, which is equal to zero. The values in the second row, corresponding to the dummy initial observation, are all equal to y.bar, except the last column, which is equal to one, al of which are divided by \\(\\lambda_4\\).\nThe following R function prior.ex() generates vectors and matrices by which \\(\\bf y\\) and \\(\\bf X\\) should be extended.\n\nprior.ex &lt;- function(data, p = 1, lambda_3 = 1, lambda_4 = 1){\n  \n  N = 1\n  M = p + 1\n  \n  y.bar     = mean(data[1:p])\n    \n  Y_star    = rbind((y.bar)/lambda_3, y.bar/lambda_4)\n  X_star    = as.matrix(c(rep(0, N), 1/lambda_4))\n  for (i in 1:p) {\n    X_star  = cbind(Y_star, X_star)\n  }\n  \n  ext.data &lt;- list(YN = Y_star, XN = X_star)\n  \n  return(ext.data)\n  \n  \n}"
  },
  {
    "objectID": "index.html#student-t-error-term",
    "href": "index.html#student-t-error-term",
    "title": "Bayesian Autoregressions",
    "section": "Student-\\(t\\) error term",
    "text": "Student-\\(t\\) error term\nA possible extension to the model is to relax the assumption of normally distributed errors. By assuming a t-distribution for the errors we can better account for excess kurtosis in the data, also referred to as ‘fat tails’. Such a specification lends itself well to estimating models on data commonly found to be leptokurtic, such as financial time series.\n\\[ u_t \\sim \\mathcal{t}(0,\\Sigma,\\nu) \\]\n\nScale Mixture of Normals Representation of the \\(t\\) Distribution\nImplementation of t-distributed errors can be achieved by representing the t-distribution as a ‘scale mixture of normal distributions’.\nIf, \\[\n\\begin{align*}\n  x|\\lambda &\\sim \\mathcal{N}(0, \\lambda\\sigma^2) \\\\\n  \\lambda &\\sim \\mathcal{IG2}(s, \\nu)\n\\end{align*}\n\\] then \\[ x \\sim t(0, s, \\sigma^2, \\nu) \\] This result can be derived by expressing the joint distribution of \\(x\\) and \\(\\lambda\\) as \\[ p(x,\\lambda) = p(x|\\lambda) p(\\lambda) \\] and then intergrating with respect to \\(\\lambda\\)\n\\[ \\int_0^\\infty p(x|\\lambda) p(\\lambda) d\\lambda \\]\nThis integration produces a density function which is proportional to the distribution for a Student-t distribution with \\(\\nu\\) degrees of freedom. Hence, \\(x\\sim t_\\nu\\).\n\n\nLikelihood and Posteriors under Student-t errors\nImplementing the scale mixture specification, the conditional distribution of \\(y\\) now takes the following form\n\\[ \\boldsymbol{y}|\\boldsymbol{X},\\boldsymbol{\\alpha},\\sigma^2,\\lambda \\sim \\mathcal{N_T}(\\boldsymbol{X}\\boldsymbol{\\alpha}, \\sigma^2\\lambda I_T) \\]\nThe likelihood under this specification now becomes\n\\[ L(\\boldsymbol{\\alpha}, \\sigma^2, \\lambda | \\boldsymbol{y}, \\boldsymbol{X}) = (2\\pi)^{-\\frac{T}{2}} (\\sigma^2)^{-\\frac{T}{2}} \\det(\\lambda I_T)^{-\\frac{1}{2}}\nexp(-\\frac{1}{2}\\frac{1}{\\sigma^2}(\\boldsymbol{y - X\\alpha})'(\\lambda I_T)^{-1}(\\boldsymbol{y - X\\alpha}))\\]\nAs before, we can derive the full conditional posteriors of the parameters by multiplying the likelihood and the priors. The natural conjugacy of \\(\\boldsymbol\\alpha\\) and \\(\\sigma^2\\) are preserved under this specification and therefore the joint posterior is given by\n\\[ p(\\boldsymbol{\\alpha}, \\sigma^2|\\boldsymbol{y, X}, \\lambda) \\propto L(\\boldsymbol{\\alpha}, \\sigma^2, \\lambda | \\boldsymbol{y}, \\boldsymbol{X}) p(\\boldsymbol{\\alpha}, \\sigma^2) \\]\nExpanding and rearranging the resulting expression allows the posterior to be expressed in the form of a Normal Inverse Gamma 2 with the following moments\n\\[\\begin{align*}\np(\\boldsymbol{\\alpha}, \\sigma^2|\\boldsymbol{\\mathbf{y}, \\mathbf{X}}, \\lambda) &= \\mathcal{NIG2}(\\boldsymbol{\\overline\\alpha, \\overline V_\\alpha}, \\overline s, \\overline \\nu) \\\\\n\\overline{\\boldsymbol\\alpha} &= \\overline{\\mathbf{V}}_{\\boldsymbol\\alpha}( \\underline{\\mathbf{V}}^{-1}_{\\boldsymbol\\alpha}\\underline{\\boldsymbol\\alpha}+\\mathbf{X}'(\\lambda I_T)^{-1}\\mathbf{y})\\\\\n\\overline{\\mathbf{V}}_{\\boldsymbol\\alpha} &=\\left(\\underline{\\mathbf{V}}^{-1}_{\\boldsymbol\\alpha}+\\mathbf{X}'(\\lambda I_T)^{-1}\\mathbf{X}\\right)^{-1} \\\\\n\\overline{s} &= \\underline{s}+\\overline{\\boldsymbol\\alpha}'\\overline{\\mathbf{V}}^{-1}_{\\boldsymbol\\alpha}\\overline{\\boldsymbol\\alpha}+\\underline{\\boldsymbol\\alpha}'\\underline{\\mathbf{V}}^{-1}_{\\boldsymbol\\alpha}\\underline{\\boldsymbol\\alpha}+\\mathbf{y}'(\\lambda I_T)^{-1}\\mathbf{y}\\\\\n\\overline{\\nu} &= \\underline{\\nu}+T\n\\end{align*}\\]\nThe full conditional for \\(\\lambda\\) is also derived in a similar manner, with the posterior distribution taking the form of an Inverse Gamma 2\n\\[\\begin{align*}\np(\\lambda|\\mathbf{y,X},\\boldsymbol{\\alpha},\\sigma^2) &\\propto L(\\boldsymbol{\\alpha}, \\sigma^2, \\lambda | \\mathbf{y, X})p(\\lambda) \\\\\n&\\propto (\\sigma^2)^{-\\frac{T}{2}}\\det(\\lambda I_T)^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}\\frac{1}{\\sigma^2}(\\mathbf{y - X\\alpha})'(\\lambda I_T)^{-1}(\\mathbf{y - X\\alpha})) \\\\\n&\\times \\lambda^{-\\frac{\\underline \\nu_\\lambda +2}{2}}\\exp(-\\frac{1}{2}\\frac{\\underline s_\\lambda}{\\lambda})\n\\end{align*}\\]\nRearranging the above to express in the form of \\(\\mathcal{IG2}\\)\n\\[\\begin{align*}\np(\\lambda|\\mathbf{y,X},\\boldsymbol{\\alpha},\\sigma^2) &= \\mathcal{IG2}(\\overline s_\\lambda, \\overline \\nu_\\lambda) \\\\\n\\overline s_\\lambda &= \\frac{1}{\\sigma^2}(\\mathbf{y - X\\boldsymbol\\alpha})'(\\mathbf{y - X\\boldsymbol\\alpha}) + \\underline s_\\lambda \\\\\n\\overline \\nu_\\lambda &= T + \\underline \\nu_\\lambda\n\\end{align*}\\]\n\n\nGibbs Sampler Code for t-distributed errors\nNow that we have the full conditionals for all of the parameters we can incorporate them into the Gibbs Sampler routine in order to draw a sample from the posterior densities.\nThe routine proceeds as follows:\nInitialize lambda based on priors for \\(\\underline s_\\lambda\\) and \\(\\underline \\nu_\\lambda\\).\nAt each iteration s:\n\nDraw \\(\\sigma^{2(s)}\\) from the \\(\\mathcal{IG2}(\\overline S, \\overline\\nu)\\) distribution\nDraw \\(\\mathbf{\\alpha^{(s)}}\\) from the \\(\\mathcal{N}(\\boldsymbol{\\overline \\alpha}, \\sigma^{2(s)} \\overline V)\\) distribution\nDraw \\(\\lambda^{(s)}\\) from \\(\\mathcal{IG2}(\\overline s_\\lambda, \\overline\\nu_\\lambda)\\)\n\nThe sampling routine is implemented via the following function in R and the results of applying the sampler to the artificial random walk data are displayed. For the purposes of this simulation a relatively large prior for the degrees of freedom parameter is chosen, which communicates our prior belief that the data should follow a random walk.\n\nTDist.Gibbs.sampler = function(S, Y, X, priors){\n  # A function to sample from the full conditionals of alpha, sigma and lambda\n  #\n  # priors: a list containing the usual priors for alpha, Sigma, S and nu\n  # as well as additional priors for lambda hyper-parameters lambda_s and lambda_nu.\n  \n  A.prior = priors$alpha\n  A_V.gprior = priors$Sigma\n  Sigma_s.prior = priors$S\n  Sigma_v.prior = priors$nu\n  lambda_s.prior = priors$lambda_s\n  lambda_v.prior = priors$lambda_nu\n  \n  lambda.draw = lambda_s.prior/rchisq(1, lambda_v.prior)\n  \n  Sigma.posterior.draws = array(NA, c(1,1,S))\n  A.posterior.draws = array(NA, c(k,1,S))\n  lambda.posterior.draws = rep(NA,S)\n  for (s in 1:S){\n    \n    lambda.gprior.diag = diag(lambda.draw, nrow(Y))\n    \n    A_V.posterior     = solve(t(X)%*%diag(1/diag(lambda.gprior.diag))%*%X + solve(A_V.gprior))\n    A.posterior       = A_V.posterior%*%(t(X)%*%diag(1/diag(lambda.gprior.diag))%*%Y + solve(A_V.gprior)%*%A.prior)\n    Sigma_s.posterior = t(Y)%*%diag(1/diag(lambda.gprior.diag))%*%Y + t(A.prior)%*%solve(A_V.gprior)%*%A.prior + Sigma_s.prior - t(A.posterior)%*%solve(A_V.posterior)%*%A.posterior\n    Sigma_v.posterior = nrow(Y) + Sigma_v.prior\n    \n    #Sigma.inv.draw      = rWishart(1, Sigma_v.posterior, solve(Sigma_s.posterior))[,,1]\n    #Sigma.posterior.draws[,,s] = solve(Sigma.inv.draw)\n    \n    Sigma.posterior.draws[,,s] = Sigma_s.posterior/ rchisq(1, df=Sigma_v.posterior)\n    Sigma.inv.draw = solve(Sigma.posterior.draws[,,s])\n    \n    A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.posterior), sigma=Sigma.posterior.draws[,,s]%x%A_V.posterior), ncol=2)\n    \n    lambda_s.posterior = sum(diag(Sigma.inv.draw%*%t(Y - X%*%A.posterior.draws[,,s])%*%(Y - X%*%A.posterior.draws[,,s]))) + lambda_s.prior\n    lambda_v.posterior = nrow(Y)*2 + lambda_v.prior\n    \n    lambda.draw = lambda_s.posterior / rchisq(1, lambda_v.posterior)\n    lambda.posterior.draws[s] = lambda.draw\n  }\n  return(list(A.posterior.draws, Sigma.posterior.draws, lambda.posterior.draws))\n}\n\ntdist.res = TDist.Gibbs.sampler(1000, Y, X, c(priors, list(lambda_s=30, lambda_nu=30)))\n\n#Alpha posterior mean:\nround(apply(tdist.res[[1]], 1:2, mean),2)\n\n      [,1]\n[1,] -0.10\n[2,]  0.98\n\n#Sigma posterior mean:\nround(mean(tdist.res[[2]]),2)\n\n[1] 4.18\n\n#Lambda posterior mean:\nround(mean(tdist.res[[3]]),2)\n\n[1] 0.25"
  },
  {
    "objectID": "index.html#estimating-autoregressions-after-2020",
    "href": "index.html#estimating-autoregressions-after-2020",
    "title": "Bayesian Autoregressions",
    "section": "Estimating autoregressions after 2020",
    "text": "Estimating autoregressions after 2020\nThe 2020 COVID-19 pandemic significantly altered the global economic landscape. It may be argued that the pre and post COVID periods are not easily comparable, or that the most severe changes for COVID would best be discounted due to their warping effect on the overall data. However, Lenza and Primiceri (2022) disagree. In their paper How to estimate a vector autoregression after March 2020, rather than discount data, they suggest that the effects of COVID be modeled as a temporary spike in volatility. They found that the first three periods of the pandemic are where volatility is the most unpredictable and this finding holds regardless of whether monthly or quarterly data is being used. For higher frequency data the exact effect is unknown though should cover at least the first 3 months of the pandemic. Thus, they propose the following change to the standard formula for autoregressions:\n\\[y_t = \\mathbf{x}_t'\\boldsymbol\\alpha + c_tu_t,\\]\nwhere \\(c_t\\) is a standard deviation multiplier. That is, for every period before COVID it takes a value of 1, for the first 3 periods of COVID it takes values \\(\\bar{c}_0\\), \\(\\bar{c}_1\\), \\(\\bar{c}_2\\), and then in all future periods a value of \\[c_{t*+j} = 1 + (\\bar{c}_2 - 1)\\rho^{j-2}, \\] where \\(\\rho\\in(0,1)\\) captures the exponential decay in the value of the conditional standard deviation towards the value one. This creates a vector that leaves the error term unchanged before COVID, has a surge in volatility during COVID, and then decays geometrically after COVID. This structure approximates the observed shocks to volatility and facilitates straightforward estimation.\nBy dividing both sides by \\(c_t\\) the model equation can be rewritten as \\[\\tilde{y}_t = \\tilde{\\mathbf{x}}_t'\\boldsymbol\\alpha + u_t\\] where \\(\\tilde{y}_t = y_t/c_t\\) and \\(\\tilde{\\mathbf{x}}_t = \\mathbf{x}_t/c_t\\). These simple transformations then lend themselves to analysis using whatever estimation method is preferred.\nThe main difficulty arises in the estimation of \\(c_t\\) as it is an unknown parameter in many cases.\n\nMethodology\nTo estimate the values for \\(c_0\\), \\(c_1\\), and \\(c_2\\) define a \\(T\\)-vector \\(\\mathbf{c}\\) of COVID volatility variables:\n\\[\\mathbf{c}=\\begin{bmatrix}1&\\dots& c_0 & c_1 & c_2 & 1+(c_2-1)\\rho &  1+(c_2-1)\\rho^2&\\dots\\end{bmatrix}'\\]\nIntuitively, volatility variable for the period before COVID is set to unity. Heightened volatility during the first three quarters of COVID are parameterized, then they are assumed to decay at a geometric rate beginning at the fourth quarter since the pandemic’s onset.\nThese COVID volatility parameters are collected in a vector \\(\\boldsymbol\\theta=(c_0\\quad c_1\\quad c_2\\quad\\rho)\\) and are estimated from their own marginal posterior as proposed by Lenza and Primiceri (2022):\n\\[p(\\boldsymbol\\theta\\mid\\mathbf{y},\\mathbf{X})\\propto P(\\mathbf{y}\\mid\\mathbf{X},\\boldsymbol\\theta)p(\\boldsymbol\\theta)\\]\nwhere the likelihood function is given as:\n\\[\\begin{align}p(\\mathbf{y},\\mathbf{X}|\\boldsymbol\\theta) &\\propto \\Bigg(\\prod^T_{t=1}c_t^{-N}\\Bigg)\\underline{\\mathbf{s}}^{\\frac{\\underline{\\nu}}{2}}\\det\\left(\\underline{\\mathbf{V}}\\right)^{-\\frac{1}{2}}\\det\\left(\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}+\\underline{\\mathbf{V}}^{-1}\\right)^{-\\frac{1}{2}}\\\\\n&\\det\\left(\\underline{\\mathbf{s}}+\\hat{\\tilde{\\mathbf{u}}}'\\hat{\\tilde{\\mathbf{u}}}+(\\hat{\\tilde{\\boldsymbol\\alpha}}-\\underline{\\boldsymbol\\alpha})'\\underline{\\mathbf{V}}^{-1}(\\hat{\\tilde{\\boldsymbol\\alpha}}-\\underline{\\boldsymbol\\alpha})\\right)^{-\\frac{T-p+\\underline{\\nu}}{2}},\\end{align}\\] where \\(\\tilde{\\mathbf{X}} = \\begin{bmatrix}\\tilde{\\mathbf{x}}_1' & \\dots & \\tilde{\\mathbf{x}}_T' \\end{bmatrix}'\\), \\(\\tilde{\\mathbf{y}} = \\begin{bmatrix}\\tilde{y}_1 & \\dots & \\tilde{y}_T \\end{bmatrix}'\\), \\(\\hat{\\tilde{\\mathbf{u}}} = \\tilde{\\mathbf{y}} - \\tilde{\\mathbf{X}}\\hat{\\tilde{\\boldsymbol\\alpha}}\\), and \\(\\hat{\\tilde{\\boldsymbol\\alpha}}=(\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}-\\underline{\\mathbf{V}}^{-1})^{-1}(\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{y}}-\\underline{\\mathbf{V}}^{-1}\\underline{\\boldsymbol\\alpha})\\).\nand the priors are assumed to be: \\[\\begin{align}\n\\mathbf{c}_0,\\mathbf{c}_1,\\mathbf{c}_2 &\\sim \\mathcal{P}areto(1,1)\\\\\n\\rho&\\sim \\mathcal{B}eta(3,1.5)\n\\end{align}\\]\n\n\nAlgorithm with code\nThis first step is to sample draws of the vector of COVID volatility variables \\(\\textbf{c}\\), which is done by estimating parameters in \\(\\boldsymbol\\theta=(\\bar{c}_0,\\bar{c}_1,\\bar{c}_2,\\rho)\\) by sampling from their own marginal posterior via a Metropolis MCMC algorithm:\n\nInitialize \\(\\boldsymbol\\theta\\) at the posterior mode which is can be located via numerical optimization. We executed this in R through the following function, c.posterior.mode. This function takes in data representing matrix \\(\\textbf{y}\\) and returns values for \\(\\boldsymbol\\theta\\) that minimize its negative log-posterior, as well as the corresponding Hessian.\n\n\nc.posterior.mode &lt;- function(data, p=4, k1=1, k2=100, start_date=c(1991,1)){\n\n  v.neglogPost &lt;- function(theta){\n    N = ncol(data)\n    K = 1 + p*N\n    Y       = ts(data[(p+1):nrow(data),], start=start_date, frequency=4)\n    T = nrow(Y)\n    X       = matrix(1,T,1)\n    # nrow(X)\n    for (i in 1:p){\n      X     = cbind(X,data[(p+1):nrow(data)-i,])\n    }\n    \n    # Calculate MLE for prior \n    ############################################################\n    A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\n    Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n    \n    # Specify prior distribution\n    ############################################################\n    kappa.1     = k1\n    kappa.2     = k2\n    kappa.3     = 1\n    A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))\n    A.prior[2:(N+1),] = kappa.3*diag(N)\n    V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))\n    S.prior     = diag(diag(Sigma.hat))\n    nu.prior    = N+1\n    \n    vec &lt;- theta[1:3]\n    for (i in 4:12){\n      vec &lt;- c(vec, 1 + (theta[3]-1)*theta[4]^(i-3))\n    }  \n    \n    V &lt;- c(ts(rep(1, nrow(Y)-12), c(1991,1), frequency = 4) , vec)    \n    \n    Y.tilde &lt;- diag(1/V)%*%Y\n    X.tilde &lt;- diag(1/V)%*%X\n    A.tilde.hat &lt;- solve((t(X.tilde)%*%X.tilde+solve(V.prior)))%*%(t(X.tilde)%*%Y.tilde+solve(V.prior)%*%A.prior)\n    epsilon.tilde &lt;-Y.tilde - X.tilde%*%A.tilde.hat\n    \n    # Log-likelihood      \n    logL &lt;- log(prod(V^(-N)))+(-N/2)*log(det(t(X.tilde)%*%X.tilde+solve(V.prior)))+\n            (-(T-p+nu.prior)/2)*log(det(S.prior +t(epsilon.tilde)%*%epsilon.tilde + \n            t(A.tilde.hat-A.prior)%*%solve(V.prior)%*%(A.tilde.hat-A.prior)))\n    \n    # Pareto(1,1) and Beta(3,1.5) priors \n    pareto.a=1\n    pareto.b=1\n    beta.a=3\n    beta.b=1.5\n    beta.cons &lt;- 1/beta(beta.a,beta.b)\n    \n    # Log-prior\n    logP &lt;- log((pareto.a*pareto.b^pareto.a)/(theta[1]^(pareto.a+1))*\n    (pareto.a*pareto.b^pareto.a)/(theta[2]^(pareto.a+1))*\n    (pareto.a*pareto.b^pareto.a)/(theta[3]^(pareto.a+1))*\n    beta.cons*theta[4]^(beta.a-1)*(1-theta[4])^(beta.b-1))\n    \n    # negative log-posterior\n    neglogPost &lt;- -(logL+logP)\n    \n    return(neglogPost)\n  }\n   \n  # numerically minimize the negative log-likelihood\n  post.maximizer &lt;- optim(par=c(50, 50, 50, 0.5), fn=v.neglogPost, method=\"L-BFGS-B\", \n                          lower=c(1, 1, 1, 0.0001),\n                          upper=c(100,100,100,0.99999), hessian = TRUE)\n  \n  return(list(maximizer=post.maximizer$par, hessian=post.maximizer$hessian))\n\n}\n\n\nDraw candidate \\(\\boldsymbol\\theta^{*}\\) from \\(N_4(\\boldsymbol\\theta^{(s-1)},cW)\\), where \\(W\\) is the inverse Hessian of the negative log posterior of \\(\\boldsymbol\\theta\\) at the mode, which is also calculated computationally, and \\(c\\) is a scaling factor.\nSet:\n\n\\[\\boldsymbol\\theta^{(s)}=  \\begin{cases}\n        \\boldsymbol\\theta^* & \\text{with pr.} \\quad \\alpha^{(s)}\n        \\\\\n        \\\\\n        \\boldsymbol\\theta^{(s-1)} & \\text{with pr.} \\quad 1-\\alpha^{(s)}\n  \\end{cases}\\]\n\\[\\alpha^{(s)} =\\text{min}\\Big[1,\\frac{p(\\boldsymbol\\theta^*|Y,X,\\underline{\\gamma})}{p(\\boldsymbol\\theta^{(s-1)}|Y,X,\\underline{\\gamma})}\\Big]\\]\n\nDefine \\(\\textbf{c}^{(s)}\\) matrix using \\(\\boldsymbol\\theta^{(s)}\\):\n\n\\[\\textbf{c}^{(s)}=[1\\quad...\\quad1 \\quad \\bar{c}_0^{(s)}\\quad \\bar{c}_1^{(s)}\\quad \\bar{c}_2^{(s)}\\quad 1+(\\bar{c}_2^{(s)}-1)\\rho^{(s)}\\quad 1+(\\bar{c}_2^{(s)}-1)\\rho^{(s)2}\\quad...]'\\]\nSteps 2 to 4 are implemented via the function, mh.mcmc which takes in data, the posterior mode of \\(\\boldsymbol\\theta\\), and the inverse Hessian from c.posterior.mode and returns the draws of \\(\\boldsymbol\\theta\\).\n\nlibrary(MASS)\nlibrary(coda)\nmh.mcmc &lt;- function(data, p=1, S.mh = 1000, c, W = diag(4), theta.init,\n                    k1 = 1, k2 = 100, start_date = c(1991,1)){\n # N = no. of variables\n  N = ncol(data)\n  # p = no. of lags\n  K = 1 + p*N\n  # forecast horizon\n  # h       = 8\n  Y       = ts(data[(p+1):nrow(data),], start=start_date, frequency=4)\n  T = nrow(Y)\n  X       = matrix(1,T,1)\n  # nrow(X)\n  for (i in 1:p){\n    X     = cbind(X,data[(p+1):nrow(data)-i,])\n  }\n  \n\n  # Calculate MLE for prior \n  ############################################################\n  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\n  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n  \n  # Specify prior distribution\n  ############################################################\n  kappa.1     = k1\n  kappa.2     = k2\n  kappa.3     = 1\n  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))\n  A.prior[2:(N+1),] = kappa.3*diag(N)\n  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))\n  S.prior     = diag(diag(Sigma.hat))\n  nu.prior    = N+1\n  \n  # Metropolis-Hastings \n  ###########################################################\n  # v0, v1, v2, rho\n  Theta &lt;- matrix(NA,S.mh,4)\n  theta_old &lt;- theta.init\n  \n  set.seed(1)\n  for (s in 1:S.mh){\n\n    covid.vec &lt;- function(theta){\n      vec &lt;- theta[1:3]\n      for (i in 4:12){\n        vec &lt;- c(vec, 1 + (theta[3]-1)*theta[4]^(i-3))\n      }\n      \n      return(vec)\n    }\n\n    # Covid volatility likelihood kernel\n    v.logL &lt;- function(V){\n      Y.tilde &lt;- diag(1/V)%*%Y\n      X.tilde &lt;- diag(1/V)%*%X\n      A.tilde.hat &lt;- solve((t(X.tilde)%*%X.tilde+solve(V.prior)))%*%(t(X.tilde)%*%Y.tilde+solve(V.prior)%*%A.prior)\n      epsilon.tilde &lt;-Y.tilde - X.tilde%*%A.tilde.hat\n\n      logL &lt;- log(prod(V^(-N)))+(-N/2)*log(det(t(X.tilde)%*%X.tilde+solve(V.prior)))+\n              (-(T-p+nu.prior)/2)*log(det(S.prior +t(epsilon.tilde)%*%epsilon.tilde + \n              t(A.tilde.hat-A.prior)%*%solve(V.prior)%*%(A.tilde.hat-A.prior)))\n\n      return(logL)\n    }\n  \n    # Covid volatility prior\n    v.logP &lt;- function(theta, pareto.a=1, pareto.b=1, beta.a=3, beta.b=1.5){\n      beta.cons &lt;- 1/beta(beta.a,beta.b)\n  \n      logP &lt;- log((pareto.a*pareto.b^pareto.a)/(theta[1]^(pareto.a+1))*\n      (pareto.a*pareto.b^pareto.a)/(theta[2]^(pareto.a+1))*\n      (pareto.a*pareto.b^pareto.a)/(theta[3]^(pareto.a+1))*\n       beta.cons*theta[4]^(beta.a-1)*(1-theta[4])^(beta.b-1))\n      \n      return(logP)\n    }\n\n    v_ones &lt;- ts(rep(1, nrow(Y)-12), c(1991,1), frequency = 4) \n    V.old &lt;- c(v_ones, covid.vec(theta_old))    \n      \n    # New candidate parameters values\n    theta_new &lt;- mvrnorm(1, theta_old, c*W)\n    V.new &lt;- c(v_ones, covid.vec(theta_new))\n    \n    # Calculate posteriors \n    v.logpost_old &lt;- v.logL(V.old)+v.logP(theta_old)\n    v.logpost_new &lt;- v.logL(V.new)+v.logP(theta_new)\n    \n    # Posterior ratio\n    post.ratio &lt;- exp(v.logpost_new-v.logpost_old)\n    \n    # Acceptance/rejection alpha\n    alpha &lt;- min(1, post.ratio)\n    \n    u_star &lt;- runif(1)\n    \n    if (alpha &gt; u_star){\n      Theta[s,] &lt;- theta_new\n    } else {Theta[s,] &lt;- theta_old}\n    \n    theta_old &lt;- Theta[s,]  \n  }\n  \n  colnames(Theta) &lt;- c(\"c0\", \"c1\" , \"c2\", \"rho\")\n\n  re &lt;- list(Theta=Theta, \n             AcceptRate = 1 - rejectionRate(as.mcmc(Theta[,1])))\n  return(re)\n}\n\n\nThese \\(\\boldsymbol\\theta\\) draws, are then used to draw \\(\\boldsymbol{\\alpha}\\) and \\(\\sigma^2\\) from the following posterior distribution:\n\n\\[p(\\boldsymbol{\\alpha}\\mid\\mathbf{y},\\mathbf{X},\\sigma^2,\\textbf{c})=\\mathcal{N}_{k}(\\bar{\\boldsymbol{\\alpha}},\\sigma^2,\\bar{\\mathbf{V}})\\] \\[p(\\sigma^2|\\mathbf{y},\\mathbf{X},\\textbf{c})=\\mathcal{IG}2(\\bar{s},\\bar{\\nu})\\\\\\]\n\\[\\bar{\\mathbf{V}}=(\\mathbf{X}'\\text{diag}(\\textbf{c}^2)^{-1}\\mathbf{X}+\\underline{\\mathbf{V}}^{-1})^{-1}\\] \\[\\bar{\\boldsymbol{\\alpha}}=\\bar{\\mathbf{V}}(\\mathbf{X}'\\text{diag}(\\textbf{c}^2)^{-1}\\mathbf{y}+\\underline{\\mathbf{V}}^{-1}\\underline{\\boldsymbol{\\alpha}})\\] \\[\\bar{\\nu}=T+\\underline{\\nu}\\] \\[\\bar{s}=\\underline{s}+\\mathbf{y}'\\text{diag}(\\textbf{c}^2)^{-1}\\mathbf{y}+\\underline{\\boldsymbol{\\alpha}}'\\underline{\\mathbf{V}}^{-1}\\underline{\\boldsymbol{\\alpha}}-\\bar{\\boldsymbol{\\alpha}}'\\bar{\\mathbf{V}}^{-1}\\bar{\\boldsymbol{\\alpha}}\\]\nThis sampling is implemented via the function covid.est below:\n\ncovid.est &lt;- function(data, p=4, S=100, k1=1, k2=100, start_date = c(1991,1), Theta.mh){\n\n  N = ncol(data)\n  K = 1 + p*N\n\n  Y       = ts(data[(p+1):nrow(data),], start=start_date, frequency=4)\n  T = nrow(Y)\n  X       = matrix(1,T,1)\n\n  for (i in 1:p){\n    X     = cbind(X,data[(p+1):nrow(data)-i,])\n  }\n  \n  covid.vec &lt;- function(theta){\n    vec &lt;- theta[1:3]\n    for (i in 4:12){\n      vec &lt;- c(vec, 1 + (theta[3]-1)*theta[4]^(i-3))\n    }\n      \n    return(vec)\n  }\n  \n  diagV.sqinv &lt;- array(NA, c(nrow(Y),nrow(Y),S))\n  \n  for (s in 1:S){\n    v_ones &lt;- ts(rep(1, nrow(Y)-12), c(1991,1), frequency = 4) \n    diagV.sqinv[,,s] &lt;- diag(c(v_ones, covid.vec(Theta.mh[s,]))^(-2))\n  }\n  \n  # Calculate MLE for prior \n  ############################################################\n  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\n  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n\n  # Specify prior distribution\n  ############################################################\n  kappa.1     = k1\n  kappa.2     = k2\n  kappa.3     = 1\n  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))\n  A.prior[2:(N+1),] = kappa.3*diag(N)\n  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))\n  S.prior     = diag(diag(Sigma.hat))\n  nu.prior    = N+1\n  \n  # Posterior draws \n  ############################################################\n  Sigma.posterior   = array(NA,c(N,N,S))\n  A.posterior       = array (NA,c(K,N,S))\n  \n  for (s in 1:S){\n    V.bar.inv   = t(X)%*%diagV.sqinv[,,s]%*%X + diag(1/diag(V.prior))\n    V.bar       = solve(V.bar.inv)\n    A.bar       = V.bar%*%(t(X)%*%diagV.sqinv[,,s]%*%Y + diag(1/diag(V.prior))%*%A.prior)\n    nu.bar      = nrow(Y) + nu.prior\n    S.bar       = S.prior + t(Y)%*%diagV.sqinv[,,s]%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%\n                  A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\n    S.bar.inv   = solve(S.bar)\n    L                 = t(chol(V.bar))\n    \n    # RF posterior draws\n    Sigma.posterior[,,s] &lt;- solve(rWishart(1, df=nu.bar, Sigma=S.bar.inv)[,,1])\n    cholSigma.s     = chol(Sigma.posterior[,,s])\n    A.posterior[,,s]       = matrix(mvrnorm(1,as.vector(A.bar), Sigma.posterior[,,s]%x%V.bar),ncol=N)\n    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s\n  }\n\n  re &lt;- list(\"A.posterior\"=A.posterior, \"Sigma.posterior\"=Sigma.posterior, \"Theta\"= Theta.mh)\n  return(re)\n}"
  },
  {
    "objectID": "index.html#stochastic-volatility-heteroskedasticity",
    "href": "index.html#stochastic-volatility-heteroskedasticity",
    "title": "Bayesian Autoregressions",
    "section": "Stochastic volatility heteroskedasticity",
    "text": "Stochastic volatility heteroskedasticity"
  },
  {
    "objectID": "index.html#analytical-results-for-one-period-ahead-forecast",
    "href": "index.html#analytical-results-for-one-period-ahead-forecast",
    "title": "Bayesian Autoregressions",
    "section": "Analytical results for one-period-ahead forecast",
    "text": "Analytical results for one-period-ahead forecast\nIn the frequentist approach, we condition the predictive density on the parameters \\(\\left(\\boldsymbol\\alpha\\right)\\), the parameters. The latter are treated as non-random. Therefore, it can be stated that this approach to forecasting ignores estimation uncertainty.\nTo address this issue, the Bayesian approach treats the parameters \\(\\boldsymbol\\alpha\\) and \\(\\sigma^2\\) as random variables and thus, it incorporates estimation uncertainty into the predictive density. Additionally, by imposing appropriate priors Bayesian forecasting often leads to more precise forecasts as it narrows the predictive intervals.\nMoreover, Bayesian forecasting uses the conditional predictive density as a starting point. In the following formula for the conditional one-period ahead forecast, the first expression under the integral is the conditional predictive density and the second expression is the joint posterior distribution that we are now used to derive.\n\\[\\begin{align}\np\\left(y _{t+1} \\mid \\mathbf{y}, \\mathbf{X}\\right) &=\\iint p\\left(y _{t+1} \\mid\\mathbf{y}, \\mathbf{X}, \\boldsymbol\\alpha, \\sigma^2\\right) p\\left(\\boldsymbol\\alpha, \\sigma^2 \\mid \\mathbf{y}, \\mathbf{X}\\right) d \\boldsymbol\\alpha d \\sigma^2\\\\\np\\left(y _{t+1} \\mid\\mathbf{y}, \\mathbf{X}, \\boldsymbol\\alpha, \\sigma^2 \\right) &=\\mathcal{N} \\left(x_{t+1}^{\\prime} \\boldsymbol\\alpha, \\sigma^2\\right)\\\\\np\\left(\\boldsymbol\\alpha, \\sigma^2 \\mid \\mathbf{y}, \\mathbf{X}\\right) &=p\\left(\\boldsymbol\\alpha \\mid \\mathbf{y}, \\mathbf{X}, \\sigma^2\\right) p\\left(\\sigma^2 \\mid \\mathbf{y}, \\mathbf{X}\\right)\\\\\np\\left(\\boldsymbol\\alpha \\mid \\mathbf{y}, \\mathbf{X}, \\sigma^2 \\right) &= \\mathcal{N}_{K}\\left(\\overline{\\boldsymbol\\alpha}, \\sigma^2\\overline{\\mathbf{V}}\\right)\\\\\np\\left(\\sigma^2 \\mid \\mathbf{y}, \\mathbf{X}\\right) &=\\mathcal{IG}2(\\overline{s}, \\overline{\\nu})\n\\end{align}\n\\] To derive the solution of this expression analytically, we integrate the parameters out in this order:\n\nWe integrate out \\(\\boldsymbol\\alpha\\) and obtain:\n\n\\[\np\\left(y_{t+1} \\mid \\mathbf{y}, \\mathbf{X}, \\sigma^2\\right)=\\mathcal{N}\\left(\\mathbf{x}_{t+1}^{\\prime} \\overline{\\boldsymbol\\alpha}, \\sigma^2\\left(1+\\mathbf{x}_{t+1}^{\\prime} \\overline{\\mathbf{V}} \\mathbf{x}_{t+1}\\right)\\right)\n\\]\n\nWe integrate out \\(\\sigma^2\\) and obtain the one-period-ahead predictive density given by a Student-\\(t\\) distribution as defined by Bauwens, Lubrano, and Richard (1999): \\[\np\\left(y_{t+1} \\mid \\mathbf{y}, \\mathbf{X}\\right)=t\\left(\\mathbf{x}_{t+1}^{\\prime} \\overline{\\boldsymbol\\alpha}, 1+\\mathbf{x}_{t+1}^{\\prime} \\overline{\\mathbf{V}} \\mathbf{x}_{t+1}, \\overline{s}, \\overline{\\nu}\\right).\n\\]"
  },
  {
    "objectID": "index.html#numerical-approach-to-one-period-ahead-forecast",
    "href": "index.html#numerical-approach-to-one-period-ahead-forecast",
    "title": "Bayesian Autoregressions",
    "section": "Numerical approach to one-period-ahead forecast",
    "text": "Numerical approach to one-period-ahead forecast\nThe Bayesian approach of the conditional predictive density is a combination of a conditional predictive density and the posterior distribution of the unknown parameters \\(\\boldsymbol\\alpha\\) and \\(\\sigma^{2}\\). The one-period ahead conditional predictive density can be expressed as below.\n\\[\np(y_{t+1} |x_{t+1},\\mathbf{y}, \\mathbf{X}) = \\int\\int p(y_{t+1} |x_{t+1}, \\mathbf{y}, \\mathbf{X}, \\boldsymbol\\alpha, \\sigma^{2})p(\\boldsymbol\\alpha, \\sigma^{2}|\\mathbf{y},\\mathbf{X}) d\\boldsymbol\\alpha d\\sigma^{2}\\]\n\\[p(y_{t+1} |x_{t+1}, \\mathbf{y},\\mathbf{X}, \\boldsymbol\\alpha, \\sigma^{2}) = \\mathcal{N}( \\alpha'_{d}d_{t+1|t} + \\alpha_{1}y_{t}+\\dots+\\alpha_{p}y_{t-p+1}, \\sigma^{2})\\]\n\\[p(\\boldsymbol\\alpha, \\sigma^{2}|\\mathbf{y},\\mathbf{X}) = \\mathcal{NIG}2(\\overline{\\boldsymbol\\alpha}, \\overline{\\sigma^{2}}, \\overline{S}, \\overline{v}) \\]\nTo sample the joint predictive density, we integrate out \\(\\boldsymbol\\alpha\\) and \\(\\sigma^{2}\\). Below steps show how the one-period ahead joint predictive density can be obtained.\nSample draws from the posterior distribution \\(p(\\boldsymbol\\alpha, \\sigma^{2}|\\bf y,\\bf X)\\) and\nObtain \\(\\left\\{ \\boldsymbol\\alpha^{(s)}, \\sigma^{2(s)} \\right\\}^{S}_{s=1}\\)\nFor each draw of \\(\\boldsymbol\\alpha\\) and \\(\\sigma^{2}\\) sample from \\(p(y_{t+1} |x_{t+1})\\) that is specified as \\(y_{t+1} \\sim \\mathcal{N} (x_{t+1|t}\\boldsymbol\\alpha^{(s)}, \\sigma^{2(s)})\\)\nObtain \\(\\left\\{ y_{t+1|t}^{(s)} \\right\\}^{S}_{s=1}\\)\nCharacterise the predictive density using \\(\\left\\{ y_{t+1|t}^{(s)} \\right\\}^{S}_{s=1}\\)"
  },
  {
    "objectID": "index.html#forecasting-many-periods-ahead",
    "href": "index.html#forecasting-many-periods-ahead",
    "title": "Bayesian Autoregressions",
    "section": "Forecasting many periods ahead",
    "text": "Forecasting many periods ahead"
  },
  {
    "objectID": "index.html#sampler-implementation-in-r",
    "href": "index.html#sampler-implementation-in-r",
    "title": "Bayesian Autoregressions",
    "section": "Sampler implementation in R",
    "text": "Sampler implementation in R\n\n## Apply function when needed\nposterior.sample.draws = posterior.draws(S=50000, Y=Y, X=X)         # using the posterior function to draw alpha and sigma\nA.posterior.simu       = posterior.sample.draws$A.posterior         # obtain posterior alpha\nSigma.posterior.simu   = posterior.sample.draws$Sigma.posterior     # obtain posterior sigma\n\n## forecasting setup\nh                      = 12                                         # specify the desired number of steps ahead\nS                      = 50000                                      # the number of sampling time, no greater than the simulation time in the posterior draw function\nY.h                    = matrix(NA,h,S)                             # create empty matrix to store the h-step ahead Y \n\n## sampling predictive density\nfor (s in 1:S){\n  A.posterior.draw     = A.posterior.simu[,s]\n  Sigma.posterior.draw = Sigma.posterior.simu[,s]\n    x.Ti               = Y[(nrow(Y)-p+1):nrow(Y)]                   # the number of lags in Y\n    x.Ti               = x.Ti[p:1]\n  for (i in 1:h){\n    x.T                = c(d,as.vector(t(x.Ti)))                    # d refers to d row vector for the deterministic term data\n    Y.f                = rnorm(1, mean = x.T%*%A.posterior.draw, sigma=Sigma.posterior.draw)\n      x.Ti             = rbind(Y.f,x.Ti[1:(p-1)])                   # refresh the initial data in Y\n    Y.h[i,s]           = Y.f\n  }\n}"
  }
]